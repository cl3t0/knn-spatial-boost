[
  {
    "objectID": "Brazil Real Estate Pricing/research.html",
    "href": "Brazil Real Estate Pricing/research.html",
    "title": "Brazil Real Estate Pricing Research",
    "section": "",
    "text": "I want to predict future prices for houses and apartments. To do this, I plan to build a model to predict price given latitude, longitude, and a set of features (like number of rooms, square footage, etc) and a date in the range of my data (2013 to 2016) and another model to build this forecast to current prices (2023) and future prices (2030, for example).\nIn this research, I will focus on the first model. The next one will be developed in the future.\nTo do price calculation I developed a “new” approach due to the lack of spatial features. Here I’m going to call it “KNN Spatial Boost”. I tested 5 models:\nThe “KNN Spatial Boost” is a boosting algorithm to improve other model’s performance in spatial data. It’s almost a feature engineering technique because it adds k-nearest neighbors (based on specific features) as features but in addition, randomizes those neighbors a little bit in the training routine to reduce the model dependency on the dataset and improve generalization.\nTo read more about the algorithm, visit the code repository."
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#data-exploration-data-cleaning",
    "href": "Brazil Real Estate Pricing/research.html#data-exploration-data-cleaning",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Data Exploration & Data Cleaning",
    "text": "Data Exploration & Data Cleaning\nHere’s my initial dataset. We need to check for outliers, missing fields, and see some distributions to identify data skewness.\n\nraw_df = pd.read_csv(\"properati-BR-2016-11-01-properties-sell.csv\", sep=\",\")\nraw_df.head(3)\n\n\n\n\n\n\n\n\ncreated_on\noperation\nproperty_type\nplace_name\nplace_with_parent_names\ngeonames_id\nlat-lon\nlat\nlon\nprice\n...\nsurface_covered_in_m2\nprice_usd_per_m2\nprice_per_m2\nfloor\nrooms\nexpenses\nproperati_url\ndescription\ntitle\nimage_thumbnail\n\n\n\n\n0\n2013-04-25\nsell\napartment\nMondubim\n|Brasil|Ceará|Fortaleza|Mondubim|\nNaN\nNaN\nNaN\nNaN\n155900.0\n...\nNaN\nNaN\nNaN\nNaN\n2.0\nNaN\nhttp://mondubim.properati.com.br/px9_vende-se_...\nOtimo Imovel com o melhor valor da regiao, con...\nApartamento Em Fortaleza\nhttps://thumbs-cf.properati.com/8/EY670SQWML7c...\n\n\n1\n2013-04-25\nsell\nhouse\nManhuaçu\n|Brasil|Minas Gerais|Manhuaçu|\nNaN\nNaN\nNaN\nNaN\n950000.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nhttp://manhuacu.properati.com.br/pxv_vende-se_...\nOtimo Imovel com o melhor valor da regiao, con...\nCasa Em Manhuacu\nhttps://thumbs-cf.properati.com/1/1VGQees9LIbx...\n\n\n2\n2013-04-25\nsell\nhouse\nIbatiba\n|Brasil|Espírito Santo|Ibatiba|\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nhttp://ibatiba.properati.com.br/pxw_vende-se_o...\nOtimo Imovel com o melhor valor da regiao, con...\nSítio Em Ibatiba\nNaN\n\n\n\n\n3 rows × 24 columns\n\n\n\n\nraw_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 872672 entries, 0 to 872671\nData columns (total 24 columns):\n #   Column                      Non-Null Count   Dtype  \n---  ------                      --------------   -----  \n 0   created_on                  872672 non-null  object \n 1   operation                   872672 non-null  object \n 2   property_type               872672 non-null  object \n 3   place_name                  872672 non-null  object \n 4   place_with_parent_names     872672 non-null  object \n 5   geonames_id                 140 non-null     float64\n 6   lat-lon                     387227 non-null  object \n 7   lat                         387227 non-null  float64\n 8   lon                         387227 non-null  float64\n 9   price                       819401 non-null  float64\n 10  currency                    819382 non-null  object \n 11  price_aprox_local_currency  819401 non-null  float64\n 12  price_aprox_usd             819401 non-null  float64\n 13  surface_total_in_m2         216934 non-null  float64\n 14  surface_covered_in_m2       633240 non-null  float64\n 15  price_usd_per_m2            589181 non-null  float64\n 16  price_per_m2                589181 non-null  float64\n 17  floor                       50794 non-null   float64\n 18  rooms                       541746 non-null  float64\n 19  expenses                    233318 non-null  float64\n 20  properati_url               872672 non-null  object \n 21  description                 872672 non-null  object \n 22  title                       872672 non-null  object \n 23  image_thumbnail             834007 non-null  object \ndtypes: float64(13), object(11)\nmemory usage: 159.8+ MB\n\n\nOur feature selection process will be based on non-null so we are going to select\n\ncreated_on\nproperty_type\nlat\nlon\nprice\nsurface_covered_in_m2\nrooms\n\nI would love to use floor and surface_total_in_m2 as features but they are filled for less than 10% for the dataset. In the future, the description column can be used with NLP to improve the price prediction accuracy.\nBut before selecting them we need to make some other analysis. For example, which categories do we have in property_type and in currency?\n\nfig, ax = plt.subplots(2, figsize=(6, 14))\n\nax[0].title.set_text(\"Property type distribution\")\nax[0].set_ylabel(\"quantity\")\nraw_df[raw_df.property_type.notnull()].property_type.value_counts().sort_index().plot(\n    kind=\"bar\", ax=ax[0], xlabel=\"\"\n)\nplt.setp(ax[0].get_xticklabels(), rotation=30, horizontalalignment=\"right\")\n\nax[1].title.set_text(\"Currency distribution\")\nax[1].set_ylabel(\"quantity\")\nraw_df[raw_df.currency.notnull()].currency.value_counts().sort_index().plot(\n    kind=\"bar\", ax=ax[1], xlabel=\"\"\n)\nplt.setp(ax[1].get_xticklabels(), rotation=30, horizontalalignment=\"right\")\nfig.show()\n\n/tmp/ipykernel_212015/2208899610.py:16: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\nUnbalanced values can cause problems in model performance. Because of this we are going to drop PH, stores, and non-BRL rows. For continuous features, we need to do a similar analysis.\nIn addition to feature selection, we will convert the created_on column to ordinal and convert the property_type column to two columns like is_house and is_apartment (one-hot encoding).\n\nclean_df = raw_df[\n    raw_df.created_on.notnull()\n    & raw_df.lat.notnull()\n    & raw_df.lon.notnull()\n    & raw_df.price.notnull()\n    & raw_df.rooms.notnull()\n    & raw_df.surface_covered_in_m2.notnull()\n    & (raw_df.currency == \"BRL\")\n    & ((raw_df.property_type == \"apartment\") | (raw_df.property_type == \"house\"))\n][\n    [\n        \"lat\",\n        \"lon\",\n        \"created_on\",\n        \"property_type\",\n        \"price\",\n        \"surface_covered_in_m2\",\n        \"rooms\",\n    ]\n]\nclean_df.created_on = clean_df.created_on.map(datetime.datetime.fromisoformat).map(\n    datetime.datetime.toordinal\n)\nclean_df.loc[:, \"is_house\"] = 0\nclean_df.loc[clean_df.property_type == \"house\", \"is_house\"] = 1\n\nclean_df.loc[:, \"is_apartment\"] = 0\nclean_df.loc[clean_df.property_type == \"apartment\", \"is_apartment\"] = 1\n\nclean_df.drop(columns=[\"property_type\"], inplace=True)\nclean_df.head(3)\n\n\n\n\n\n\n\n\nlat\nlon\ncreated_on\nprice\nsurface_covered_in_m2\nrooms\nis_house\nis_apartment\n\n\n\n\n792\n-3.474983\n-38.928616\n735045\n5000000.0\n1000.0\n8.0\n1\n0\n\n\n794\n-23.547697\n-46.657379\n735115\n2000000.0\n180.0\n3.0\n0\n1\n\n\n795\n-23.545349\n-46.659519\n735115\n960000.0\n109.0\n3.0\n0\n1\n\n\n\n\n\n\n\nNow the same analysis as before but now for continuous features.\n\ndef group_values(series: pd.Series, group_size: float) -&gt; pd.Series:\n    return ((series / group_size).map(int) * group_size).value_counts().sort_index()\n\n\nfig, ax = plt.subplots(nrows=7, ncols=2, figsize=(12, 42))\n\nax[0, 0].title.set_text(\"Price distribution\")\nax[0, 0].set_xlabel(\"price\")\nax[0, 0].set_ylabel(\"quantity\")\nax[0, 0].plot(group_values(clean_df[clean_df.price &gt; 0].price, 50000))\n\nax[0, 1].title.set_text(\"log10(Price) distribution\")\nax[0, 1].set_xlabel(\"log10(price)\")\nax[0, 1].set_ylabel(\"quantity\")\nax[0, 1].plot(group_values(np.log10(clean_df[clean_df.price &gt; 0].price), 0.5))\n\nax[1, 0].title.set_text(\"Surface covered in m2 distribution\")\nax[1, 0].set_xlabel(\"m2\")\nax[1, 0].set_ylabel(\"quantity\")\nax[1, 0].plot(\n    group_values(\n        clean_df[\n            (clean_df.surface_covered_in_m2 &gt; 0)\n            & (clean_df.surface_covered_in_m2 &lt; 5000)\n        ].surface_covered_in_m2,\n        100,\n    )\n)\n\nax[1, 1].title.set_text(\"log10(Surface covered in m2) distribution\")\nax[1, 1].set_xlabel(\"log(m2)\")\nax[1, 1].set_ylabel(\"quantity\")\nax[1, 1].plot(\n    group_values(\n        np.log10(\n            clean_df[\n                (clean_df.surface_covered_in_m2 &gt; 0)\n                & (clean_df.surface_covered_in_m2 &lt; 5000)\n            ].surface_covered_in_m2\n        ),\n        0.1,\n    )\n)\n\npositive_price_and_m2 = clean_df[\n    (clean_df.price &gt; 0) & (clean_df.surface_covered_in_m2 &gt; 0)\n]\n\nfiltered_price_and_m2 = positive_price_and_m2[\n    (positive_price_and_m2.price / positive_price_and_m2.surface_covered_in_m2 &gt; 1500)\n    & (\n        positive_price_and_m2.price / positive_price_and_m2.surface_covered_in_m2\n        &lt; 20000\n    )\n]\n\nax[2, 0].title.set_text(\"Price/surface covered in m2 distribution\")\nax[2, 0].set_xlabel(\"price/m2\")\nax[2, 0].set_ylabel(\"quantity\")\nax[2, 0].plot(\n    group_values(\n        positive_price_and_m2.price / positive_price_and_m2.surface_covered_in_m2,\n        500,\n    )\n)\n\nax[2, 1].title.set_text(\"Price/surface covered in m2 distribution but filtered\")\nax[2, 1].set_xlabel(\"price/m2\")\nax[2, 1].set_ylabel(\"quantity\")\nax[2, 1].plot(\n    group_values(\n        filtered_price_and_m2.price / filtered_price_and_m2.surface_covered_in_m2,\n        500,\n    )\n)\n\nax[3, 0].title.set_text(\"Rooms distribution\")\nax[3, 0].set_xlabel(\"rooms\")\nax[3, 0].set_ylabel(\"quantity\")\nax[3, 0].plot(group_values(clean_df.rooms, 1))\n\nax[3, 1].title.set_text(\"Rooms distribution but filtered\")\nax[3, 1].set_xlabel(\"rooms\")\nax[3, 1].set_ylabel(\"quantity\")\nax[3, 1].plot(group_values(clean_df[clean_df.rooms &lt; 16].rooms, 1))\n\nax[4, 0].title.set_text(\"Creation data distribution\")\nax[4, 0].set_xlabel(\"days since 01-01-0001\")\nax[4, 0].set_ylabel(\"quantity\")\nax[4, 0].plot(\n    group_values(\n        clean_df.created_on,\n        100,\n    )\n)\n\nax[4, 1].title.set_text(\"log(Creation data) distribution\")\nax[4, 1].set_xlabel(\"log(days since 01-01-0001)\")\nax[4, 1].set_ylabel(\"quantity\")\nax[4, 1].plot(\n    group_values(\n        np.log10(clean_df.created_on),\n        0.00001,\n    )\n)\n\nax[5, 0].title.set_text(\"Latitude distribution\")\nax[5, 0].set_xlabel(\"latitude\")\nax[5, 0].set_ylabel(\"quantity\")\nax[5, 0].plot(group_values(clean_df.lat, 1))\n\nax[5, 1].title.set_text(\"Latitude distribution but filtered\")\nax[5, 1].set_xlabel(\"latitude\")\nax[5, 1].set_ylabel(\"quantity\")\nax[5, 1].plot(group_values(clean_df[clean_df.lat &lt; -2].lat, 1))\n\nax[6, 0].title.set_text(\"Longitude distribution\")\nax[6, 0].set_xlabel(\"longitude\")\nax[6, 0].set_ylabel(\"quantity\")\nax[6, 0].plot(group_values(clean_df.lon, 1))\n\nax[6, 1].title.set_text(\"Longitude distribution but filtered\")\nax[6, 1].set_xlabel(\"longitude\")\nax[6, 1].set_ylabel(\"quantity\")\nax[6, 1].plot(\n    group_values(clean_df[(clean_df.lon &lt; -34) & (clean_df.lon &gt; -55)].lon, 1)\n)\n\nfig.show()\n\n/tmp/ipykernel_212015/2237906502.py:129: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\nNote that some of those features are worth applying some non-linear normalization like logarithm. The price and the surface covered in m2 are obviously skewed. Rooms, creation date, price over m2, latitude, and longitude are not so much so I’m only going to apply some linear transformation and filtering to them. Before adding new features I want to see feature correlation to check if they make sense.\n\ncorr_df = clean_df.copy()\ncorr_df[\"log_price\"] = np.log10(corr_df.price)\ncorr_df[\"log_m2\"] = np.log10(corr_df.surface_covered_in_m2)\ncorr_df[\"price_over_m2\"] = corr_df.price / corr_df.surface_covered_in_m2\ncorr_df[\"log_price_over_m2\"] = np.log10(corr_df.price / corr_df.surface_covered_in_m2)\ncorr = corr_df.corr().round(2)\nsb.heatmap(corr, cmap=\"Blues\", annot=True, annot_kws={\"fontsize\": 8})\n\n/home/cleto/repos/knn-spatial-boost/.venv/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log10\n  result = getattr(ufunc, method)(*inputs, **kwargs)\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nSince price and surface_covered_in_m2 are skewed I considered applying the log to them. But note that log_price and log_m2 are highly correlated (0.74). If we take a look at the price_over_m2 row in the correlation matrix, we notice that it’s not highly correlated with anything and price_over_m2 distribution is not skewed. Because of this, we are going to use it as the target column!\n\ndf = corr_df[\n    (corr_df.lat &lt; -2)\n    & (corr_df.lon &lt; -34)\n    & (corr_df.lon &gt; -55)\n    & (corr_df.price &gt; 0)\n    & (corr_df.rooms &lt; 16)\n    & (corr_df.price &gt; 0)\n    & (corr_df.surface_covered_in_m2 &gt; 0)\n    & (corr_df.surface_covered_in_m2 &lt; 5000)\n    & (corr_df.price_over_m2 &gt; 1500)\n    & (corr_df.price_over_m2 &lt; 20000)\n][[\"lat\", \"lon\", \"created_on\", \"rooms\", \"is_house\", \"is_apartment\", \"price_over_m2\"]]\ndf.head(3)\n\n\n\n\n\n\n\n\nlat\nlon\ncreated_on\nrooms\nis_house\nis_apartment\nprice_over_m2\n\n\n\n\n792\n-3.474983\n-38.928616\n735045\n8.0\n1\n0\n5000.000000\n\n\n794\n-23.547697\n-46.657379\n735115\n3.0\n0\n1\n11111.111111\n\n\n795\n-23.545349\n-46.659519\n735115\n3.0\n0\n1\n8807.339450\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 153337 entries, 792 to 872640\nData columns (total 7 columns):\n #   Column         Non-Null Count   Dtype  \n---  ------         --------------   -----  \n 0   lat            153337 non-null  float64\n 1   lon            153337 non-null  float64\n 2   created_on     153337 non-null  int64  \n 3   rooms          153337 non-null  float64\n 4   is_house       153337 non-null  int64  \n 5   is_apartment   153337 non-null  int64  \n 6   price_over_m2  153337 non-null  float64\ndtypes: float64(4), int64(3)\nmemory usage: 9.4 MB\n\n\nHere’s a geographical view of the data. For future comparison, we are going to “zoom in” Belo Horizonte (BH), which is a big city, filter to only apartments with 1 room. I used logarithm in the price to show colors in a smoother way.\n\nbh_lats = (-19.99491, -19.83163)\nbh_longs = (-44.02857, -43.90612)\n\n\nbh_df = df[\n    (df.lat &gt; bh_lats[0])\n    & (df.lat &lt; bh_lats[1])\n    & (df.lon &gt; bh_longs[0])\n    & (df.lon &lt; bh_longs[1])\n    & (df.is_apartment == 1)\n    & (df.rooms == 1)\n]\nplt.scatter(\n    bh_df[\"lon\"],\n    bh_df[\"lat\"],\n    s=20,\n    c=bh_df[\"price_over_m2\"],\n    cmap=\"plasma_r\",\n    alpha=0.8,\n)\nplt.colorbar()\nplt.title(\"Belo Horizonte price distribution\")\nplt.xlabel(\"Longitude\")\nplt.ylabel(\"Latitude\")\nplt.show()\n\n\n\n\n\nmin_created_on = df.created_on.min()\nmax_created_on = df.created_on.max()\nmin_lat = df.lat.min()\nmax_lat = df.lat.max()\nmin_lon = df.lon.min()\nmax_lon = df.lon.max()\nmin_rooms = df.rooms.min()\nmax_rooms = df.rooms.max()\nmin_lon = df.lon.min()\nmax_lon = df.lon.max()\nmin_price_over_m2 = df.price_over_m2.min()\nmax_price_over_m2 = df.price_over_m2.max()\n\n\ndef normalize_xs(x: pd.DataFrame) -&gt; pd.DataFrame:\n    x.created_on = (x.created_on - min_created_on) / (max_created_on - min_created_on)\n    x.lat = (x.lat - min_lat) / (max_lat - min_lat)\n    x.lon = (x.lon - min_lon) / (max_lon - min_lon)\n    x.rooms = (x.rooms - min_rooms) / (max_rooms - min_rooms)\n    return x\n\n\ndef normalize_y(x: pd.DataFrame) -&gt; pd.DataFrame:\n    x.price_over_m2 = (x.price_over_m2 - min_price_over_m2) / (\n        max_price_over_m2 - min_price_over_m2\n    )\n    return x\n\n\ndef denormalize_y(y: np.ndarray) -&gt; pd.DataFrame:\n    return (min_price_over_m2 + y * (max_price_over_m2 - min_price_over_m2)).round(2)\n\n\ndef prepare(x: pd.DataFrame) -&gt; pd.DataFrame:\n    new_x = x.copy()\n    return normalize_xs(normalize_y(new_x))\n\n\nprepared_df = prepare(df)\nprepared_df.head(3)\n\n\n\n\n\n\n\n\nlat\nlon\ncreated_on\nrooms\nis_house\nis_apartment\nprice_over_m2\n\n\n\n\n792\n0.964141\n0.793751\n0.000000\n0.500000\n1\n0\n0.189147\n\n\n794\n0.242964\n0.407479\n0.057236\n0.142857\n0\n1\n0.519595\n\n\n795\n0.243049\n0.407372\n0.057236\n0.142857\n0\n1\n0.395022\n\n\n\n\n\n\n\nBefore starting training, we are going to split our dataset into a training dataset and a validation dataset.\n\ndef split_dataset(\n    dataset: pd.DataFrame, test_ratio: float = 0.30\n) -&gt; t.Tuple[pd.DataFrame, pd.DataFrame]:\n    test_indices = np.random.rand(len(dataset)) &lt; test_ratio\n    return dataset[~test_indices], dataset[test_indices]\n\n\ntrain, valid = split_dataset(prepared_df)\nX_train = train.loc[:, train.columns != \"price_over_m2\"].to_numpy()\ny_train = train.price_over_m2.to_numpy()\nX_valid = valid.loc[:, valid.columns != \"price_over_m2\"].to_numpy()\ny_valid = valid.price_over_m2.to_numpy()\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_valid.shape)\nprint(y_valid.shape)\n\n(107424, 6)\n(107424,)\n(45913, 6)\n(45913,)"
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#sanity-checks",
    "href": "Brazil Real Estate Pricing/research.html#sanity-checks",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Sanity checks",
    "text": "Sanity checks\nEven with the validation dataset, we want to make some sanity checks after the training. To do it so, we built this function which evaluates the model with specific features. It’s a curried function because there are some standardized checks to apply to all models.\n\ndef eval(\n    lat: t.Union[float, np.ndarray],\n    lon: t.Union[float, np.ndarray],\n    created_on: t.Union[datetime.date, t.List[datetime.date]],\n    property_type: t.Literal[\"apartment\", \"house\"],\n    rooms: t.Union[int, np.ndarray],\n) -&gt; t.Callable[[t.Any], pd.DataFrame]:\n    lat_vec = lat if isinstance(lat, np.ndarray) else np.array([lat])\n    lon_vec = lon if isinstance(lon, np.ndarray) else np.array([lon])\n    created_on_vec = (\n        np.array([c.toordinal() for c in created_on])\n        if isinstance(created_on, t.List)\n        else np.array([created_on.toordinal()])\n    )\n    rooms_vec = rooms if isinstance(rooms, np.ndarray) else np.array([rooms])\n\n    m = np.vstack(\n        [\n            v.ravel()\n            for v in np.meshgrid(\n                lat_vec,\n                lon_vec,\n                created_on_vec,\n                rooms_vec,\n                indexing=\"ij\",\n            )\n        ]\n    ).T\n\n    is_apartment = int(property_type == \"apartment\")\n    is_house = int(property_type == \"house\")\n\n    x_df = pd.DataFrame(\n        {\n            \"lat\": m[:, 0],\n            \"lon\": m[:, 1],\n            \"created_on\": m[:, 2],\n            \"is_apartment\": [is_apartment] * len(m),\n            \"is_house\": [is_house] * len(m),\n            \"rooms\": m[:, 3],\n        }\n    )\n    x = normalize_xs(x_df.copy()).to_numpy()\n\n    def fn(model: t.Any) -&gt; pd.DataFrame:\n        y = model.predict(x)\n        x_df[\"price_over_m2\"] = denormalize_y(y.reshape(len(m)))\n        return x_df\n\n    return fn\n\nHere are all sanity checks:\n\nlats = np.linspace(bh_lats[0], bh_lats[1], 100)\nlongs = np.linspace(bh_longs[0], bh_longs[1], 50)\n\nsantacruz = (-19.87755, -43.94184)\nsion = (-19.95408, -43.93163)\n\nstart = datetime.date(2013, 7, 1)\ndate_list = [start + datetime.timedelta(days=x) for x in range(365 * 3)]\n\nbh_heatmap_check = eval(lats, longs, datetime.date(2016, 1, 1), \"apartment\", 1)\nsanta_cruz_price_evolution = eval(\n    santacruz[0], santacruz[1], date_list, \"apartment\", np.array([1, 2, 3])\n)\nsion_price_evolution = eval(\n    sion[0], sion[1], date_list, \"apartment\", np.array([1, 2, 3])\n)\n\nHere’s a function to plot everything once:\n\ndef sanity_checks(model: t.Any, price_limit: t.Optional[float] = None) -&gt; None:\n    bh_heatmap = bh_heatmap_check(model)\n    santa_cruz = santa_cruz_price_evolution(model)\n    sion = sion_price_evolution(model)\n\n    if price_limit is not None:\n        bh_heatmap.loc[bh_heatmap.price &gt; price_limit, \"price\"] = price_limit\n\n    fig, ax = plt.subplots(3, figsize=(6, 13))\n    fig.tight_layout(pad=5.0)\n\n    ax[0].title.set_text(\"BH price heatmap\")\n    ax[0].set_xlabel(\"Longitude\")\n    ax[0].set_ylabel(\"Latitude\")\n    scatter = ax[0].scatter(\n        bh_heatmap[\"lon\"],\n        bh_heatmap[\"lat\"],\n        s=20,\n        c=bh_heatmap[\"price_over_m2\"],\n        cmap=\"plasma_r\",\n        alpha=0.8,\n        **(\n            {\"norm\": colors.Normalize(vmin=0, vmax=price_limit)}\n            if price_limit is not None\n            else {}\n        )\n    )\n    fig.colorbar(scatter)\n    plt.setp(ax[0].get_xticklabels(), rotation=30, horizontalalignment=\"right\")\n\n    ax[1].title.set_text(\"Santa Cruz price/m2 evolution\")\n    ax[1].set_xlabel(\"Date\")\n    ax[1].set_ylabel(\"Price/m2\")\n    ax[1].plot(\n        date_list,\n        santa_cruz[santa_cruz.rooms == 1].price_over_m2.rolling(30).mean(),\n        label=\"1 room\",\n    )\n    ax[1].plot(\n        date_list,\n        santa_cruz[santa_cruz.rooms == 2].price_over_m2.rolling(30).mean(),\n        label=\"2 rooms\",\n    )\n    ax[1].plot(\n        date_list,\n        santa_cruz[santa_cruz.rooms == 3].price_over_m2.rolling(30).mean(),\n        label=\"3 rooms\",\n    )\n    ax[1].legend()\n    plt.setp(ax[1].get_xticklabels(), rotation=30, horizontalalignment=\"right\")\n\n    ax[2].title.set_text(\"Sion price/m2 evolution\")\n    ax[2].set_xlabel(\"Date\")\n    ax[2].set_ylabel(\"Price/m2\")\n    ax[2].plot(\n        date_list,\n        sion[sion.rooms == 1].price_over_m2.rolling(30).mean(),\n        label=\"1 room\",\n    )\n    ax[2].plot(\n        date_list,\n        sion[sion.rooms == 2].price_over_m2.rolling(30).mean(),\n        label=\"2 rooms\",\n    )\n    ax[2].plot(\n        date_list,\n        sion[sion.rooms == 3].price_over_m2.rolling(30).mean(),\n        label=\"3 rooms\",\n    )\n    ax[2].legend()\n    plt.setp(ax[2].get_xticklabels(), rotation=30, horizontalalignment=\"right\")\n\n    fig.show()"
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#random-forest",
    "href": "Brazil Real Estate Pricing/research.html#random-forest",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Random Forest",
    "text": "Random Forest\nWe are going to give a try to the Random Forest model.\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=0)\n\nrf.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=15, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=15, random_state=0)\n\n\n\nrf.score(X_valid, y_valid)\n\n0.7391105641168709\n\n\n\nsanity_checks(rf)\n\n/tmp/ipykernel_212015/1983228620.py:73: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\nEven with the high score, the model behavior is absolutely bad.\nBH price heatmap: The model struggled to learn information from coordinates and built rectangular patterns. To get a more precise rectangular segmentation we need to increase max_depth and n_estimators to allow the model to store more information about the coordinates.\nPrice evolution: In both plots, the model cannot see the price difference between 1, 2, and 3 rooms. In real life every location got more expensive from 2013 to 2016 but for the model, the price doesn’t go up.\nI believe it happened because of three reasons:\n\nLack of generalization capabilities in the RF model;\nLack of data;\nRF can learn a limited amount of information given max_depth and n_estimators. To increase it is necessary to raise max_depth and n_estimators. Unfortunately, it will increase the model size (which currently is 127 MB), increase time to train, increase time to inference, increase RAM usage, and can overfit the model.\n\nGoing beyond this point is not worth it. It makes RF not suitable for geospatial data with only coordinates."
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#knn",
    "href": "Brazil Real Estate Pricing/research.html#knn",
    "title": "Brazil Real Estate Pricing Research",
    "section": "KNN",
    "text": "KNN\nWe are going to give it a try to KNN.\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nknn = KNeighborsRegressor(n_neighbors=15, weights=\"distance\")\nknn.fit(X_train, y_train)\n\nKNeighborsRegressor(n_neighbors=15, weights='distance')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsRegressorKNeighborsRegressor(n_neighbors=15, weights='distance')\n\n\n\nknn.score(X_valid, y_valid)\n\n0.6321752472080835\n\n\n\nsanity_checks(knn)\n\n/tmp/ipykernel_212015/1983228620.py:73: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\nLow score and bad model behavior.\nBH price heatmap: The model struggled to learn information from coordinates (maybe because of other features that are not geospatial coordinates) and built a random surface. Even lowering our expectations and allowing for a “low-resolution heatmap” the price should be higher at the bottom of the figure, not lower.\nPrice evolution: In both plots, the model cannot see too much price difference between 1, 2 and 3 rooms. The price is the same for a well-known expensive location and a well-known cheap location. In real life, every location got more expensive from 2013 to 2016 but for the model, the price doesn’t go up. Instead of it, the price goes up and down “randomly”.\nThis model can “learn” a lot of information but cannot learn the patterns from them."
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#neural-networks",
    "href": "Brazil Real Estate Pricing/research.html#neural-networks",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Neural Networks",
    "text": "Neural Networks\nWe are going to give it a try to neural networks.\n\nfrom sklearn.neural_network import MLPRegressor\n\n\nnn = MLPRegressor(\n    hidden_layer_sizes=(128, 64, 32, 16),\n    verbose=True,\n    max_iter=200,\n    batch_size=32,\n    early_stopping=True,\n    random_state=0,\n)\n\nnn.fit(X_train, y_train)\n\nIteration 1, loss = 0.01155141\nValidation score: 0.221862\nIteration 2, loss = 0.01088603\nValidation score: 0.212346\nIteration 3, loss = 0.01077097\nValidation score: 0.243658\nIteration 4, loss = 0.01061189\nValidation score: 0.262032\nIteration 5, loss = 0.01034798\nValidation score: 0.275889\nIteration 6, loss = 0.00965983\nValidation score: 0.314949\nIteration 7, loss = 0.00913479\nValidation score: 0.402566\nIteration 8, loss = 0.00878743\nValidation score: 0.365734\nIteration 9, loss = 0.00846887\nValidation score: 0.323008\nIteration 10, loss = 0.00835023\nValidation score: 0.457188\nIteration 11, loss = 0.00822268\nValidation score: 0.450064\nIteration 12, loss = 0.00813812\nValidation score: 0.429624\nIteration 13, loss = 0.00803822\nValidation score: 0.208194\nIteration 14, loss = 0.00809019\nValidation score: 0.464554\nIteration 15, loss = 0.00811037\nValidation score: 0.381254\nIteration 16, loss = 0.00806553\nValidation score: 0.448898\nIteration 17, loss = 0.00805290\nValidation score: 0.461965\nIteration 18, loss = 0.00788982\nValidation score: 0.475674\nIteration 19, loss = 0.00793062\nValidation score: 0.449990\nIteration 20, loss = 0.00778544\nValidation score: 0.486660\nIteration 21, loss = 0.00779338\nValidation score: 0.477434\nIteration 22, loss = 0.00777171\nValidation score: 0.462441\nIteration 23, loss = 0.00770208\nValidation score: 0.382629\nIteration 24, loss = 0.00781632\nValidation score: 0.490394\nIteration 25, loss = 0.00778360\nValidation score: 0.444528\nIteration 26, loss = 0.00771133\nValidation score: 0.465044\nIteration 27, loss = 0.00769576\nValidation score: 0.471903\nIteration 28, loss = 0.00773866\nValidation score: 0.449037\nIteration 29, loss = 0.00770919\nValidation score: 0.494166\nIteration 30, loss = 0.00771471\nValidation score: 0.465014\nIteration 31, loss = 0.00767140\nValidation score: 0.467776\nIteration 32, loss = 0.00757540\nValidation score: 0.487759\nIteration 33, loss = 0.00759566\nValidation score: 0.490186\nIteration 34, loss = 0.00758822\nValidation score: 0.410153\nIteration 35, loss = 0.00759636\nValidation score: 0.501076\nIteration 36, loss = 0.00757625\nValidation score: 0.497474\nIteration 37, loss = 0.00761486\nValidation score: 0.440051\nIteration 38, loss = 0.00756604\nValidation score: 0.456769\nIteration 39, loss = 0.00751279\nValidation score: 0.465779\nIteration 40, loss = 0.00759903\nValidation score: 0.450521\nIteration 41, loss = 0.00755100\nValidation score: 0.498528\nIteration 42, loss = 0.00751146\nValidation score: 0.503334\nIteration 43, loss = 0.00753971\nValidation score: 0.489244\nIteration 44, loss = 0.00751878\nValidation score: 0.481944\nIteration 45, loss = 0.00752564\nValidation score: 0.484779\nIteration 46, loss = 0.00749715\nValidation score: 0.474444\nIteration 47, loss = 0.00754483\nValidation score: 0.362897\nIteration 48, loss = 0.00749922\nValidation score: 0.379450\nIteration 49, loss = 0.00744500\nValidation score: 0.481598\nIteration 50, loss = 0.00744425\nValidation score: 0.490585\nIteration 51, loss = 0.00747556\nValidation score: 0.492221\nIteration 52, loss = 0.00743051\nValidation score: 0.503634\nIteration 53, loss = 0.00746039\nValidation score: 0.497111\nIteration 54, loss = 0.00743607\nValidation score: 0.506194\nIteration 55, loss = 0.00738568\nValidation score: 0.474368\nIteration 56, loss = 0.00740268\nValidation score: 0.493748\nIteration 57, loss = 0.00750992\nValidation score: 0.449288\nIteration 58, loss = 0.00735858\nValidation score: 0.491898\nIteration 59, loss = 0.00737670\nValidation score: 0.476540\nIteration 60, loss = 0.00734037\nValidation score: 0.432427\nIteration 61, loss = 0.00732651\nValidation score: 0.520696\nIteration 62, loss = 0.00734420\nValidation score: 0.515336\nIteration 63, loss = 0.00734858\nValidation score: 0.470467\nIteration 64, loss = 0.00734688\nValidation score: 0.482842\nIteration 65, loss = 0.00731572\nValidation score: 0.504744\nIteration 66, loss = 0.00740405\nValidation score: 0.491073\nIteration 67, loss = 0.00736960\nValidation score: 0.515602\nIteration 68, loss = 0.00734922\nValidation score: 0.477252\nIteration 69, loss = 0.00731116\nValidation score: 0.499762\nIteration 70, loss = 0.00727751\nValidation score: 0.411075\nIteration 71, loss = 0.00731228\nValidation score: 0.521941\nIteration 72, loss = 0.00735339\nValidation score: 0.509044\nIteration 73, loss = 0.00726377\nValidation score: 0.507393\nIteration 74, loss = 0.00731905\nValidation score: 0.444809\nIteration 75, loss = 0.00723431\nValidation score: 0.521621\nIteration 76, loss = 0.00729300\nValidation score: 0.516061\nIteration 77, loss = 0.00727376\nValidation score: 0.519239\nIteration 78, loss = 0.00725786\nValidation score: 0.508508\nIteration 79, loss = 0.00729266\nValidation score: 0.510332\nIteration 80, loss = 0.00726435\nValidation score: 0.496192\nIteration 81, loss = 0.00718731\nValidation score: 0.511710\nIteration 82, loss = 0.00723880\nValidation score: 0.375131\nValidation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n\n\nMLPRegressor(batch_size=32, early_stopping=True,\n             hidden_layer_sizes=(128, 64, 32, 16), random_state=0,\n             verbose=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPRegressorMLPRegressor(batch_size=32, early_stopping=True,\n             hidden_layer_sizes=(128, 64, 32, 16), random_state=0,\n             verbose=True)\n\n\n\nnn.score(X_valid, y_valid)\n\n0.5273335291341406\n\n\n\nsanity_checks(nn)\n\n/tmp/ipykernel_341749/1983228620.py:73: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\nLow score and bad model behavior.\nBH price heatmap: The model struggled to learn information from coordinates and built a “random” surface. Even lowering our expectations and allowing a “low-resolution heatmap” the price should be higher on the right of the figure, not left.\nPrice evolution: The model ordered rooms almost OK but there’s not much price difference between 1, 2 and 3 rooms. The price is the same for a well-known expensive location and a well-known cheap location. In real life, every location got more expensive from 2013 to 2016 but for the model, the price goes down.\nTo allow the neural network to learn the patterns from the coordinates we will need a lot more parameters and running time.\nPS: I attempted to run a neural network with the architecture 6-2000-2000-2000-1 on my notebook’s graphics card, but I did not see any improvement in the score even with more than 1 hour of train time."
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#spatial-boosted-random-forest",
    "href": "Brazil Real Estate Pricing/research.html#spatial-boosted-random-forest",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Spatial Boosted Random Forest",
    "text": "Spatial Boosted Random Forest\nHere’s where things got interesting. Since the “KKN Spatial Booster” increases model spatial awareness we can get a lot better behaviour.\n\nfrom knn_spatial_boost.core import KNNSpatialBooster\n\n\nboosted_rf = KNNSpatialBooster(\n    n_neighbors=1,\n    temperature=0.5,\n    n_loops=1,\n    spatial_features=[0, 1],\n    remove_target_spatial_cols=True,\n    remove_neighbor_spatial_cols=True,\n    estimator=RandomForestRegressor(n_estimators=100, max_depth=15, random_state=0),\n)\n\nboosted_rf.fit(X_train, y_train)\n\nRunning loop #0\n\n\n\nboosted_rf.score(X_valid, y_valid)\n\n0.6462122118094535\n\n\n\nsanity_checks(boosted_rf)\n\n/tmp/ipykernel_341749/1983228620.py:73: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\nBetter model behavior but lower score.\nBH price heatmap: Now we have a picture that looks like a city and not a “random” surface. The more expensive region of the city is darker, which is a great sign.\nPrice evolution: In both plots, the model cannot see the price difference between 1, 2, and 3 rooms. In real life every location got more expensive from 2013 to 2016 but for the model, the price doesn’t go up. Instead of it, the price goes up and down “randomly”.\nEven with the spatial boost, looks like RF is not the right algorithm to deal with this problem because of its lack of generalization capabilities. It’s a calculation problem and all NNs do are calculations. That’s why we are going to try it with the spatial booster."
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#spatial-boosted-neural-network",
    "href": "Brazil Real Estate Pricing/research.html#spatial-boosted-neural-network",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Spatial Boosted Neural Network",
    "text": "Spatial Boosted Neural Network\n\nfrom sklearn.neural_network import MLPRegressor\n\n\nboosted_nn = KNNSpatialBooster(\n    n_neighbors=1,\n    temperature=0.5,\n    n_loops=10,\n    spatial_features=[0, 1],\n    remove_target_spatial_cols=True,\n    remove_neighbor_spatial_cols=True,\n    estimator=MLPRegressor(\n        hidden_layer_sizes=(64, 32, 16, 8),\n        warm_start=True,\n        early_stopping=True,\n        tol=1e-10,\n        n_iter_no_change=20,\n        verbose=True,\n        max_iter=10,\n        batch_size=32,\n        random_state=0,\n    ),\n)\n\nboosted_nn.fit(X_train, y_train)\n\nRunning loop #0\nIteration 1, loss = 0.01076201\nValidation score: 0.639482\nIteration 2, loss = 0.00505485\nValidation score: 0.646830\nIteration 3, loss = 0.00502750\nValidation score: 0.606791\nIteration 4, loss = 0.00499496\nValidation score: 0.594461\nIteration 5, loss = 0.00496672\nValidation score: 0.654910\nIteration 6, loss = 0.00493094\nValidation score: 0.651485\nIteration 7, loss = 0.00491670\nValidation score: 0.631484\nIteration 8, loss = 0.00488778\nValidation score: 0.656580\nIteration 9, loss = 0.00488642\nValidation score: 0.640114\nIteration 10, loss = 0.00487413\nValidation score: 0.654056\nRunning loop #1\nIteration 1, loss = 0.00492030\nValidation score: 0.657268\nIteration 2, loss = 0.00489820\nValidation score: 0.659917\nIteration 3, loss = 0.00490151\nValidation score: 0.657331\nIteration 4, loss = 0.00489341\nValidation score: 0.662173\nIteration 5, loss = 0.00487561\nValidation score: 0.660516\nIteration 6, loss = 0.00487458\nValidation score: 0.653273\nIteration 7, loss = 0.00486312\nValidation score: 0.663242\nIteration 8, loss = 0.00486511\nValidation score: 0.656660\nIteration 9, loss = 0.00485934\nValidation score: 0.661999\nIteration 10, loss = 0.00484734\nValidation score: 0.650012\nRunning loop #2\nIteration 1, loss = 0.00484281\nValidation score: 0.658492\nIteration 2, loss = 0.00483870\nValidation score: 0.660877\nIteration 3, loss = 0.00483310\nValidation score: 0.661852\nIteration 4, loss = 0.00482470\nValidation score: 0.661438\nIteration 5, loss = 0.00481553\nValidation score: 0.651800\nIteration 6, loss = 0.00481803\nValidation score: 0.646303\nIteration 7, loss = 0.00480763\nValidation score: 0.662768\nIteration 8, loss = 0.00480969\nValidation score: 0.663732\nIteration 9, loss = 0.00480441\nValidation score: 0.660551\nIteration 10, loss = 0.00479137\nValidation score: 0.660748\nRunning loop #3\nIteration 1, loss = 0.00482647\nValidation score: 0.657868\nIteration 2, loss = 0.00483086\nValidation score: 0.658173\nIteration 3, loss = 0.00482533\nValidation score: 0.661644\nIteration 4, loss = 0.00481965\nValidation score: 0.660866\nIteration 5, loss = 0.00481187\nValidation score: 0.654417\nIteration 6, loss = 0.00481857\nValidation score: 0.648055\nIteration 7, loss = 0.00481070\nValidation score: 0.659433\nIteration 8, loss = 0.00481106\nValidation score: 0.660187\nIteration 9, loss = 0.00480457\nValidation score: 0.660062\nIteration 10, loss = 0.00479852\nValidation score: 0.657204\nRunning loop #4\nIteration 1, loss = 0.00477730\nValidation score: 0.647587\nIteration 2, loss = 0.00477892\nValidation score: 0.657816\nIteration 3, loss = 0.00477693\nValidation score: 0.653692\nIteration 4, loss = 0.00477174\nValidation score: 0.646146\nIteration 5, loss = 0.00476712\nValidation score: 0.646889\nIteration 6, loss = 0.00477756\nValidation score: 0.657485\nIteration 7, loss = 0.00476419\nValidation score: 0.657585\nIteration 8, loss = 0.00476931\nValidation score: 0.659595\nIteration 9, loss = 0.00476752\nValidation score: 0.658798\nValidation score did not improve more than tol=0.000000 for 20 consecutive epochs. Stopping.\nRunning loop #5\nIteration 1, loss = 0.00478566\nValidation score: 0.654872\nValidation score did not improve more than tol=0.000000 for 20 consecutive epochs. Stopping.\nRunning loop #6\nIteration 1, loss = 0.00476722\nValidation score: 0.659148\nValidation score did not improve more than tol=0.000000 for 20 consecutive epochs. Stopping.\nRunning loop #7\nIteration 1, loss = 0.00477244\nValidation score: 0.645208\nValidation score did not improve more than tol=0.000000 for 20 consecutive epochs. Stopping.\nRunning loop #8\nIteration 1, loss = 0.00477162\nValidation score: 0.656917\nValidation score did not improve more than tol=0.000000 for 20 consecutive epochs. Stopping.\nRunning loop #9\nIteration 1, loss = 0.00475579\nValidation score: 0.649134\nValidation score did not improve more than tol=0.000000 for 20 consecutive epochs. Stopping.\n\n\n/home/cleto/repos/knn-spatial-boost/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/home/cleto/repos/knn-spatial-boost/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/home/cleto/repos/knn-spatial-boost/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/home/cleto/repos/knn-spatial-boost/.venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\n\nboosted_nn.score(X_valid, y_valid)\n\n0.6188857129459122\n\n\n\nsanity_checks(boosted_nn)\n\n/tmp/ipykernel_341749/1983228620.py:73: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()\n\n\n\n\n\nBest model behavior but low score.\nBH price heatmap: Now we have a picture that looks like a city and not a “random” surface. The more expensive region of the city is darker, which is a great sign.\nPrice evolution: Curves correct order (3 rooms are more expensive than 2 rooms). Correct behavior over time since the price goes up.\nEverything looks good but the scores in both training and validation datasets are poor. Note that we decided to use only the first neighbor and maybe there’s not enough context to the model to calculate a final price. To increase the number of neighbors we need to increase the NN size because it cannot handle this amount of information."
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#spatial-boosted-neural-network-bigger-architecture",
    "href": "Brazil Real Estate Pricing/research.html#spatial-boosted-neural-network-bigger-architecture",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Spatial Boosted Neural Network (Bigger Architecture)",
    "text": "Spatial Boosted Neural Network (Bigger Architecture)\nWhen trying to increase the number of neighbors and the number of neurons in each layer my CPU started struggling. The SciKit Learn implementation of NN is CPU-only so I tried Tensorflow and Keras to use my GPU.\nI tested a lot of neighbor quantities with the 2000-2000-2000-1 architecture (and built a table with training score, validation score, etc, but I lost it) and figured out that 4 is the ideal number of neighbors for that amount of neurons and layers.\n\nkeras_nn = tf.keras.models.Sequential()\nkeras_nn.add(tf.keras.layers.Dense(2000, activation=\"relu\", input_shape=(28,)))\nkeras_nn.add(tf.keras.layers.Dropout(rate=0.01))\nkeras_nn.add(tf.keras.layers.Dense(2000, activation=\"relu\"))\nkeras_nn.add(tf.keras.layers.Dense(2000, activation=\"relu\"))\nkeras_nn.add(tf.keras.layers.Dense(1, activation=\"linear\"))\n\n# Compile the model\nkeras_nn.compile(\n    loss=\"mean_squared_error\",\n    optimizer=\"adam\",\n    metrics=[tf.keras.metrics.R2Score()],\n)\n\n\nboosted_keras_nn = KNNSpatialBooster(\n    n_neighbors=4,\n    temperature=0.5,\n    n_loops=33,\n    spatial_features=[0, 1],\n    remove_target_spatial_cols=True,\n    remove_neighbor_spatial_cols=True,\n    estimator=keras_nn,\n)\n\nboosted_keras_nn.fit(X_train, y_train)\n\nRunning loop #0\n3357/3357 [==============================] - 68s 19ms/step - loss: 0.0115 - r2_score: 0.5864\nRunning loop #1\n3357/3357 [==============================] - 63s 19ms/step - loss: 0.0083 - r2_score: 0.7024\nRunning loop #2\n3357/3357 [==============================] - 63s 19ms/step - loss: 0.0082 - r2_score: 0.7041\nRunning loop #3\n3357/3357 [==============================] - 64s 19ms/step - loss: 0.0082 - r2_score: 0.7067\nRunning loop #4\n3357/3357 [==============================] - 64s 19ms/step - loss: 0.0081 - r2_score: 0.7097\nRunning loop #5\n3357/3357 [==============================] - 64s 19ms/step - loss: 0.0081 - r2_score: 0.7101\nRunning loop #6\n3357/3357 [==============================] - 64s 19ms/step - loss: 0.0080 - r2_score: 0.7121\nRunning loop #7\n3357/3357 [==============================] - 65s 19ms/step - loss: 0.0080 - r2_score: 0.7118\nRunning loop #8\n3357/3357 [==============================] - 65s 19ms/step - loss: 0.0080 - r2_score: 0.7134\nRunning loop #9\n3357/3357 [==============================] - 64s 19ms/step - loss: 0.0080 - r2_score: 0.7133\nRunning loop #10\n3357/3357 [==============================] - 65s 19ms/step - loss: 0.0080 - r2_score: 0.7131\nRunning loop #11\n3357/3357 [==============================] - 65s 19ms/step - loss: 0.0080 - r2_score: 0.7137\nRunning loop #12\n3357/3357 [==============================] - 65s 19ms/step - loss: 0.0079 - r2_score: 0.7153\nRunning loop #13\n3357/3357 [==============================] - 64s 19ms/step - loss: 0.0079 - r2_score: 0.7149\nRunning loop #14\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7156\nRunning loop #15\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7162\nRunning loop #16\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7176\nRunning loop #17\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7166\nRunning loop #18\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7164\nRunning loop #19\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7172\nRunning loop #20\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7183\nRunning loop #21\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7192\nRunning loop #22\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7186\nRunning loop #23\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7186\nRunning loop #24\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7183\nRunning loop #25\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7176\nRunning loop #26\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7180\nRunning loop #27\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7187\nRunning loop #28\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0079 - r2_score: 0.7175\nRunning loop #29\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7184\nRunning loop #30\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7182\nRunning loop #31\n3357/3357 [==============================] - 61s 18ms/step - loss: 0.0078 - r2_score: 0.7195\nRunning loop #32\n3357/3357 [==============================] - 60s 18ms/step - loss: 0.0078 - r2_score: 0.7203\n\n\n\nboosted_keras_nn.score(X_valid, y_valid)\n\n1435/1435 [==============================] - 8s 5ms/step - loss: 0.0078 - r2_score: 0.7193\n\n\n[0.007787229958921671, 0.7192660570144653]\n\n\n\nsanity_checks(boosted_keras_nn)\n\n157/157 [==============================] - 1s 3ms/step\n103/103 [==============================] - 0s 3ms/step\n103/103 [==============================] - 0s 4ms/step\n\n\n/tmp/ipykernel_341749/1983228620.py:73: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n  fig.show()"
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#conclusion",
    "href": "Brazil Real Estate Pricing/research.html#conclusion",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Conclusion",
    "text": "Conclusion\n\n\n\n\n\n\n\n\n\n\nModel\nTime to train\nScore\nBH check\nSanta Cruz and Sion check\n\n\n\n\nRandom Forest\n35.1s\n0.739\nVertical and horizontal patterns\nColapsing curves going up and down\n\n\nKNN\n0.3s\n0.632\nRandom smooth surface\nCurves going up and down\n\n\nNeural Network\n7m42.4s\n0.521\nRandom smooth surface\nCorrect curve order but they go up and down and the same price for different places\n\n\nSpatial Boosted Random Forest\n44.3s\n0.647\nSeems natural\nColapsing random curves\n\n\nSpatial Boosted Neural Network\n7m16s\n0.619\nSeems natural\nCorrect order and arguable price evolution and price difference between places\n\n\nSpatial Boosted Neural Network (Bigger Architecture)\n35m56.1s\n0.719\nSeems natural\nAlmost correct order and arguable price evolution and price difference between places\n\n\n\nRF had the best score but with a little bit more investigation we see a bad geospatial generalization and no price difference when adding more rooms. It seems to overfit but due to a lack of data and not to a model inability. We need more spatial features like distance to the nearest city center, hospital, school, public transportation, etc.\nKNN had the best time to train but struggled with all other evaluation criteria.\nNN seems to underfit. It’s expected given the time to train and the complexity of understanding price fluctuation given only coordinates. It’s possible to fit a NN with this data but only with a lot more time to train and a bigger architecture.\nInstead of adding spatial features like distance to the nearest city center, I decided to use KNN to increase model spatial awareness by adding neighbors as features. In addition, I implemented neighbor randomness to improve model generalization.\nSpatial Boosted RF had better geospatial generalization but again seems to overfit due to lack of data.\nAs NNs have greater generalization capacity, in the end, Spatial Boosted NN was the model that best managed to learn the patterns even with little data. It had the best qualitative performance even scoring worse than other models. The bigger version of it managed to score closer to the RF (0.719)."
  },
  {
    "objectID": "Brazil Real Estate Pricing/research.html#improvements-ideas",
    "href": "Brazil Real Estate Pricing/research.html#improvements-ideas",
    "title": "Brazil Real Estate Pricing Research",
    "section": "Improvements & Ideas",
    "text": "Improvements & Ideas\nI believe we can encode spatial data in better ways. As we can see here, we can use coordinates as discrete cells, calculate embeddings for them, and use them in a transformer. Maybe in the future, I will try to reproduce this technique for this problem.\n\nAbout this research\n\nGenerate a GIF to show price evolution through time.\nAdd an online demo for portfolio purposes.\ndescription column can be used with NLP.\n\n\n\nAbout the method\n\nIdentify the way this method affects the base model (needs more tuning?).\nTry another dataset to figure out which kind of problem suits better with the technique."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KNN Spatial Boost",
    "section": "",
    "text": "Using a machine learning model to obtain a huge amount of simple patterns from coordinates without any other feature becomes difficult because it depends on how much information your model can learn. KNN, as a non-parametric algorithm, can address this problem, but struggles with more complex patterns. Here’s KNN Spatial Boost algorithm comes in."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "KNN Spatial Boost",
    "section": "Install",
    "text": "Install\npip install knn_spatial_boost"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "KNN Spatial Boost",
    "section": "How to use",
    "text": "How to use\nIn the example we are going to use the MLPRegressor from the scikit-learn library.\n\nfrom knn_spatial_boost.core import KNNSpatialBooster\nfrom sklearn.neural_network import MLPRegressor\n\nnn = KNNSpatialBooster(\n    n_neighbors=2,\n    spatial_features=[1, 2],\n    estimator=MLPRegressor()\n)"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "KNNSpatialBooster\n\n KNNSpatialBooster (n_neighbors:int=5, temperature:float=0.2,\n                    n_loops:int=50, verbose:bool=False,\n                    estimator:Any=RandomForestRegressor(),\n                    estimator_output_1d_array:bool=True,\n                    spatial_features:Union[Literal['*'],List[int]]='*',\n                    remove_target_spatial_cols:bool=False,\n                    remove_neighbor_spatial_cols:bool=True)\n\nA KNN Spatial Booster.\nYou can use it with any model. It uses the training dataset to improve the spatial perception of the model adding more features. Remember to use warm_start=True if your model is a scikit-learn model.\nRead more in the KNNSpatialBooster docs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_neighbors\nint\n5\nThe number of neighbors to use as feature. Also known as “k”.\n\n\ntemperature\nfloat\n0.2\nIf temperature=0, exactly “k” nearest neighbors are selected. Iftemperature is higher, other neighbors can be selected.\n\n\nn_loops\nint\n50\nQuantity of .fit() calls. If temperature&gt;0, will create a newdataset foreach loop.\n\n\nverbose\nbool\nFalse\nIf set to True, will print the current loop.\n\n\nestimator\ntyping.Any\nRandomForestRegressor()\nAny Estimator as defined by scikit-learn with warm_start=True.\n\n\nestimator_output_1d_array\nbool\nTrue\nUsed to be compatible with estimators that needs 1d arrays in .fit().\n\n\nspatial_features\ntyping.Union[typing.Literal[’*’], typing.List[int]]\n*\nDefine which columns should be used as coordinates for KNN.\n\n\nremove_target_spatial_cols\nbool\nFalse\nSet to True if you believe the model should not use the originalspatial features in the training process.\n\n\nremove_neighbor_spatial_cols\nbool\nTrue\nSet to True if you believe the model should not use the neighborsspatial features in the training process.\n\n\n\n\n\n\nKNNSpatialBooster.fit\n\n KNNSpatialBooster.fit (X:numpy.ndarray, Y:numpy.ndarray)\n\nFit model for X and Y.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nShape must be (number of samples,) or (number of samples, number_of_features).\n\n\nY\nndarray\nShape must be (number of samples,) or (number of samples, number of outputs).\n\n\nReturns\nNone\n\n\n\n\n\n\n\nKNNSpatialBooster.predict\n\n KNNSpatialBooster.predict (X:numpy.ndarray)\n\nPredict for X.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nShape must be (number of samples,) or (number of samples, number_of_features).\n\n\nReturns\nndarray\nPredicted value for given X.\n\n\n\n\n\n\nKNNSpatialBooster.score\n\n KNNSpatialBooster.score (X:numpy.ndarray, Y:numpy.ndarray)\n\nCalculate score for X and Y. It uses the estimator score function.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nX\nndarray\nShape must be (number of samples,) or (number of samples, number_of_features).\n\n\nY\nndarray\nShape must be (number of samples,) or (number of samples, number of outputs).\n\n\nReturns\nfloat\nEstimator score.\n\n\n\n\nKNNSpatialBooster.random_first_k(10, 3, 0.5)\n\narray([[0, 2, 3],\n       [0, 2, 3],\n       [0, 1, 2],\n       [0, 3, 4],\n       [1, 2, 3],\n       [0, 1, 2],\n       [1, 3, 4],\n       [0, 1, 3],\n       [0, 2, 3],\n       [0, 1, 2]])\n\n\n\nKNNSpatialBooster.get_neighbors(\n    np.array([\n        [0, 1, 10],\n        [0, 1, 15],\n        [0, 1, 17],\n        [0, 2, 20],\n        [0, 2, 25],\n        [0, 2, 27],\n        [2, 0, 30],\n        [2, 0, 35],\n        [2, 0, 37],\n    ]),\n    np.array([\n        [20],\n        [30],\n        [35],\n        [40],\n        [50],\n        [55],\n        [60],\n        [70],\n        [75],\n    ]),\n    np.array([\n        [1, 1, 40],\n        [2, 1, 50],\n    ]),\n    [0, 1],\n    3,\n    remove_first_neighbor=True\n)\n\narray([[[10.        , 15.        , 27.        , 35.        ,\n         30.        , 25.        , 20.        ],\n        [30.        , 35.        , 17.        , 10.        ,\n         15.        , 27.        , 25.        ]],\n\n       [[20.        , 30.        , 55.        , 70.        ,\n         60.        , 50.        , 40.        ],\n        [60.        , 70.        , 35.        , 20.        ,\n         30.        , 55.        , 50.        ]],\n\n       [[ 0.5       ,  0.5       ,  0.41421356,  0.41421356,\n          0.41421356,  0.41421356,  0.41421356],\n        [ 0.5       ,  0.5       ,  0.33333333,  0.33333333,\n          0.33333333,  0.30901699,  0.30901699]]])"
  }
]